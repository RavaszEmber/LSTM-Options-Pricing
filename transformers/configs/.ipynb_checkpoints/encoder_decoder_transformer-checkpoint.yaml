model:
  name: encoder_decoder_transformer
  n_features: 5
  d_model: 64
  n_heads: 4
  n_encoder_layers: 3
  n_decoder_layers: 3
  d_ff: 256
  dropout: 0.1

data:
  path: ../data.csv
  train_months: 8
  val_months: 1
  test_month: 1
  test_year: 2023
  
  # Sequence parameters
  seq_len: 15
  label_len: 3
  pred_len: 1

  sequential: true
  group_by: ["STRIKE", "MONEYNESS_BUCKET"]  # Group by strike and moneyness
  min_chain_length: 18                       # seq_len + label_len + pred_len
  stratify_by_horizon: true                  # Track horizon distribution
  
  # Features
  feature_columns:
    - STRIKE
    - UNDERLYING_LAST
    - MTM
    - RFR
    - VOL_GG
    - DTE
    - DTE_NORMALIZED
    - MONEYNESS
  target_column: C_MID

training:
  batch_size: 64
  learning_rate: 0.0001
  weight_decay: 0.0001
  epochs: 100
  early_stopping_patience: 20
  grad_clip: 1.0
  optimizer: adam
  scheduler:
    type: reduce_on_plateau
    factor: 0.5
    patience: 10
    min_lr: 1.0e-6

hardware:
  num_workers: 4
  pin_memory: true
  seed: 42

output:
  save_dir: ./results/encoder_decoder_transformer
  log_interval: 100
