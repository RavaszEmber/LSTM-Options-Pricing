model:
  name: encoder_decoder_transformer
  n_features: 5
  d_model: 64
  n_heads: 4
  n_encoder_layers: 3
  n_decoder_layers: 3
  d_ff: 256
  dropout: 0.1

data:
  path: ../data.csv
  train_months: 8
  val_months: 1
  test_month: 1
  test_year: 2023
  
  # Sequence parameters
  seq_len: 15
  label_len: 3
  pred_len: 1

  sequential: true
  group_by: ["STRIKE", "MONEYNESS_BUCKET"]  # Group by strike and moneyness
  min_chain_length: 19                       # seq_len + label_len + pred_len
  stratify_by_horizon: true                  # Track horizon distribution
  
  # Features
  feature_columns:
    - STRIKE
    - UNDERLYING_LAST
    - MTM
    - RFR
    - VOL_GG
    - DTE
    - DTE_NORMALIZED
    - MONEYNESS
  target_column: C_MID

training:
  batch_size: 64
  learning_rate: 0.0001
  weight_decay: 0.0001
  epochs: 100
  early_stopping_patience: 20
  grad_clip: 1.0
  optimizer: adam
  scheduler:
    type: reduce_on_plateau
    factor: 0.5
    patience: 10
    min_lr: 1.0e-6
  
hyperopt:
  enabled: true
  trials: 50
  optimize_on_month: 1
  search_space:
    d_model: [24, 36, 48, 64]
    n_heads: [2, 3, 4, 6]
    n_encoder_layers: [1, 2, 3]
    n_decoder_layers: [1, 2, 3]
    dropout: [0.05, 0.1, 0.15, 0.2]
    learning_rate: [1.0e-5, 1.0e-3]
    batch_size: [32, 64, 128, 256, 512, 1024, 2048, 4096]
    weight_decay: [1.0e-6, 1.0e-3]
    seq_len: [10, 15]
    label_len: [0, 5]

hardware:
  num_workers: 4
  pin_memory: true
  seed: 42

output:
  save_dir: ./results/encoder_decoder
  log_interval: 100
