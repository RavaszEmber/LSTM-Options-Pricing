{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b321945d-4087-436c-9eeb-f143df8a6214",
   "metadata": {},
   "source": [
    "## Transformer-Based Option Pricing Implementation\n",
    "\n",
    "This notebook implements transformer architectures for option pricing based on the Informer model and comparative analysis from recent literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5174d0a-b3a3-4af3-b9a8-49f9112051bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 8\n",
      "  GPU 0: NVIDIA L4\n",
      "  GPU 1: NVIDIA L4\n",
      "  GPU 2: NVIDIA L4\n",
      "  GPU 3: NVIDIA L4\n",
      "  GPU 4: NVIDIA L4\n",
      "  GPU 5: NVIDIA L4\n",
      "  GPU 6: NVIDIA L4\n",
      "  GPU 7: NVIDIA L4\n",
      "World size: 1\n",
      "Local rank: 0\n",
      "Using device: cuda:0\n",
      "Distributed training: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Multi-GPU Training\n",
    "def setup_distributed():\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        rank = int(os.environ['RANK'])\n",
    "        world_size = int(os.environ['WORLD_SIZE'])\n",
    "        local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "        dist.init_process_group(backend='nccl')\n",
    "        torch.cuda.set_device(local_rank)\n",
    "\n",
    "        return rank, world_size, local_rank\n",
    "    else:\n",
    "        return 0, 1, 0\n",
    "\n",
    "def cleanup_distributed():\n",
    "    if dist.is_initialized():\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "rank, world_size, local_rank = setup_distributed()\n",
    "is_main_process = (rank == 0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if is_main_process:\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "        for i in range(num_gpus):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"World size: {world_size}\")\n",
    "        print(f\"Local rank: {local_rank}\")\n",
    "else:\n",
    "    if is_main_process:\n",
    "        print(\"No GPU available, using CPU\")\n",
    "\n",
    "USE_MULTI_GPU = world_size > 1\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(f'cuda:{local_rank}' if torch.cuda.is_available() else 'cpu')\n",
    "if is_main_process:\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Distributed training: {USE_MULTI_GPU}\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98387bd-6a67-4d2d-b25b-496f17c9d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration and Hyperparameters\n",
    "# Reference: Bańka & Chudziak (2025), \"Applying Informer for Option Pricing\", Section 4.2, p. 1274\n",
    "# Reference: Pimentel et al. (2025), \"Option pricing with LSTM\", Section 4.2.3, Table 2, p. [page with Table 2]\n",
    "\n",
    "CONFIG = {\n",
    "    # Data parameters - following preprocessing methodology from Preprocessing.html\n",
    "    'train_months': 8,\n",
    "    'val_months': 1,\n",
    "    'test_month': 1,\n",
    "    'test_year': 2023,\n",
    "\n",
    "    # Sequence parameters\n",
    "    'seq_len': 20,           # Input sequence length (past 20 time steps)\n",
    "    'label_len': 10,         # Decoder start token length\n",
    "    'pred_len': 1,           # Predict 1 step ahead\n",
    "    \n",
    "    # Model architecture - Informer configuration (Bańka & Chudziak, p. 1274)\n",
    "    'd_model': 36,           # Embedding dimension\n",
    "    'n_heads': 3,            # Number of attention heads\n",
    "    'n_encoder_layers': 1,   # Number of encoder layers\n",
    "    'n_decoder_layers': 2,   # Number of decoder layers\n",
    "    'd_ff': 8,               # Feedforward dimension\n",
    "    'dropout': 0.06,         # Dropout rate\n",
    "    'label_len': 5,          # Label length for decoder\n",
    "    'pred_len': 1,           # Prediction length (single day prediction)\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 64,        # As per Bańka & Chudziak, p. 1274\n",
    "    'learning_rate': 0.0001, # Adam optimizer initial LR\n",
    "    'weight_decay': 0.0001,  # L2 regularization\n",
    "    'epochs': 100,           # Maximum epochs\n",
    "    'early_stopping_patience': 20,  # Early stopping patience\n",
    "    \n",
    "    # Features - following both papers' feature selection\n",
    "    # Bańka & Chudziak, Section 3.1, p. 1272: \"strike price, implied volatility, time to maturity, underlying asset price\"\n",
    "    # Pimentel et al., Section 4.2.1: \"strike price, underlying price, time to maturity, risk-free rate, volatility\"\n",
    "    'feature_columns': ['STRIKE', 'UNDERLYING_LAST', 'MTM', 'RFR', 'VOL_GG'],\n",
    "    'target_column': 'C_MID',  # Call option price as target\n",
    "    \n",
    "    # Hyperparameter search ranges (for later optimization)\n",
    "    'hp_search': {\n",
    "        'd_model': [16, 32, 64],\n",
    "        'n_heads': [2, 3, 4],\n",
    "        'n_encoder_layers': [1, 2],\n",
    "        'n_decoder_layers': [1, 2, 3],\n",
    "        'd_ff': [8, 16, 32],\n",
    "        'dropout': [0.05, 0.06, 0.1],\n",
    "        'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "        'batch_size': [32, 64, 128]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    if key != 'hp_search':\n",
    "        print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70fd43-92f4-4b7a-bc09-514af85e8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Loading and Preparation Functions\n",
    "# Reference: Preprocessing.html, create_rolling_window_split_flexible function\n",
    "\n",
    "def load_preprocessed_data(filepath='./data.csv'):\n",
    "    \"\"\"\n",
    "    Load preprocessed options data.\n",
    "    Reference: Preprocessing.html - data preprocessing steps\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath, parse_dates=['QUOTE_DATE', 'EXPIRE_DATE'])\n",
    "        \n",
    "    print(f\"Loaded {len(df):,} call option records\")\n",
    "    print(f\"Date range: {df['QUOTE_DATE'].min()} to {df['QUOTE_DATE'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_rolling_window_split(df, test_month, test_year=2023, \n",
    "                                        feature_columns=['STRIKE', 'UNDERLYING_LAST', 'MTM', 'RFR', 'VOLATILITY'], \n",
    "                                        target_column = 'C_MID', train_months=8, val_months=1):\n",
    "    df[target_column] = df[['C_BID', 'C_ASK']].apply(pd.to_numeric, errors='coerce').mean(axis=1)\n",
    "\n",
    "    \n",
    "    test_start = pd.Timestamp(year=test_year, month=test_month, day=1)\n",
    "    test_end = test_start + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    val_start = test_start - pd.offsets.MonthBegin(val_months)\n",
    "    val_end = test_start - pd.Timedelta(days=1)\n",
    "\n",
    "    train_start = val_start - pd.offsets.MonthBegin(train_months)  \n",
    "    train_end = val_start - pd.Timedelta(days=1)\n",
    "\n",
    "    train_df = df.loc[(df['QUOTE_DATE'] >= train_start) & (df['QUOTE_DATE'] <= train_end), ['QUOTE_DATE'] + feature_columns + [target_column]].set_index('QUOTE_DATE').copy()\n",
    "    val_df = df.loc[(df['QUOTE_DATE'] >= val_start) & (df['QUOTE_DATE'] <= val_end), ['QUOTE_DATE'] + feature_columns + [target_column]].set_index('QUOTE_DATE').copy()\n",
    "    test_df = df.loc[(df['QUOTE_DATE'] >= test_start) & (df['QUOTE_DATE'] <= test_end), ['QUOTE_DATE'] + feature_columns + [target_column]].set_index('QUOTE_DATE').copy()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_df)\n",
    "\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train_df), columns=train_df.columns, index=train_df.index)\n",
    "    val_scaled = pd.DataFrame(scaler.transform(val_df), columns=val_df.columns, index=val_df.index)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns, index=test_df.index)\n",
    "    \n",
    "    return train_df, val_df, test_df, scaler\n",
    "\n",
    "# Load data\n",
    "df = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437e8e0-9796-40f7-91fc-f31fb5b2b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PyTorch Dataset Class\n",
    "\n",
    "class OptionPricingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for option pricing.\n",
    "    \n",
    "    Reference: Bańka & Chudziak (2025), Section 3.2, p. 1272-1273\n",
    "    \"The input data is structured as a time-series sequence\"\n",
    "    \"\"\"\n",
    "    def __init__(self, data_df, target_col, seq_len, label_len, pred_len, feature_cols):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: numpy array of shape (n_samples, n_features)\n",
    "            targets: numpy array of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        self.data = data_df[feature_cols].values.astype(np.float32)\n",
    "        self.target = data_df[target_col].values.astype(np.float32)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Number of valid sequences we can create\n",
    "        return len(self.data) - self.seq_len - self.label_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            x_enc: encoder input (seq_len, n_features)\n",
    "            x_dec: decoder input (label_len + pred_len, n_features)\n",
    "            y: target values (pred_len,)\n",
    "        \"\"\"\n",
    "        # Encoder input: historical sequence\n",
    "        s_begin = idx\n",
    "        s_end = s_begin + self.seq_len\n",
    "        x_enc = self.data[s_begin:s_end]\n",
    "        \n",
    "        # Decoder input: label + prediction windows\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        x_dec = self.data[r_begin:r_end]\n",
    "        \n",
    "        # Target: future values to predict\n",
    "        y = self.target[s_end:s_end + self.pred_len]\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(x_enc),\n",
    "            torch.FloatTensor(x_dec),\n",
    "            torch.FloatTensor(y)\n",
    "        )\n",
    "\n",
    "\n",
    "def create_dataloaders(data_splits, batch_size=64):\n",
    "    \"\"\"Create train, validation, and test dataloaders.\"\"\"\n",
    "    train_df = data_splits[0]\n",
    "    val_df = data_splits[1]\n",
    "    test_df = data_splits[2]\n",
    "    print(train_df)\n",
    "    \n",
    "    train_dataset = OptionPricingDataset(\n",
    "        train_df.shape, \n",
    "        np.random.randn(train_df.shape[0], 1) * 0.01\n",
    "    )\n",
    "    val_dataset = OptionPricingDataset(\n",
    "        val_df.shape,\n",
    "        np.random.randn(val_df.shape[0], 1) * 0.01\n",
    "    )\n",
    "    test_dataset = OptionPricingDataset(\n",
    "        test_df.shape,\n",
    "        np.random.randn(test_df.shape[0], 1) * 0.01\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d98e3-edb4-4c03-949e-781c40607c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Positional Encoding\n",
    "# Reference: Bańka & Chudziak (2025), Figure 2, p. 1272 (Informer architecture)\n",
    "# Reference: Vaswani et al. (2017), \"Attention is All You Need\" - base transformer architecture\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding for transformer models.\n",
    "    \n",
    "    Reference: Standard transformer architecture from Vaswani et al. (2017)\n",
    "    Used in financial time-series as noted in Bańka & Chudziak (2025), Section 3.2\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e9dd5-d5ad-43d6-9a3b-1756e894e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Simplified Transformer Model (Baseline)\n",
    "# Reference: Ruiru et al. (2024), \"LSTM versus Transformers\", Section 2.2, p. 544\n",
    "# Reference: Bańka & Chudziak (2025), Section 3, p. 1271-1273\n",
    "\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Transformer model for option pricing.\n",
    "    \n",
    "    This serves as a baseline before implementing the full Informer architecture.\n",
    "    \n",
    "    Reference: Ruiru et al. (2024), Section 2.2, Equation 4, p. 544\n",
    "    \"Attention(Q,K,V) = softmax(QK^T/√dk)V\"\n",
    "    \n",
    "    Reference: Bańka & Chudziak (2025), Figure 1, p. 1271\n",
    "    Shows conceptual overview of encoder-decoder architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, d_model=32, n_heads=4, n_layers=2, \n",
    "                 d_ff=128, dropout=0.1):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_projection = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, n_features)\n",
    "        Returns:\n",
    "            predictions: Tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Reshape to add sequence dimension\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, n_features)\n",
    "        \n",
    "        # Project to d_model dimension\n",
    "        x = self.input_projection(x)  # (batch_size, 1, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)  # (batch_size, 1, d_model)\n",
    "        \n",
    "        # Take the output and project to prediction\n",
    "        x = x.squeeze(1)  # (batch_size, d_model)\n",
    "        predictions = self.output_projection(x)  # (batch_size, 1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Transformer model for option pricing with sequence input.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, d_model=32, n_heads=4, n_layers=2, \n",
    "                 d_ff=128, dropout=0.1):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_projection = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_enc):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_enc: Input tensor of shape (batch_size, seq_len, n_features)\n",
    "        Returns:\n",
    "            predictions: Tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Project to d_model dimension\n",
    "        x = self.input_projection(x_enc)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Use the last timestep for prediction (or could use mean pooling)\n",
    "        x = x[:, -1, :]  # (batch_size, d_model)\n",
    "        \n",
    "        # Project to prediction\n",
    "        predictions = self.output_projection(x)  # (batch_size, 1)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def train_epoch_simple(model, train_loader, criterion, optimizer, device):\n",
    "        \"\"\"Training epoch for Simple Transformer\"\"\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for x_enc, x_dec, y in train_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_enc)\n",
    "            \n",
    "            output = output.squeeze()\n",
    "            y = y.squeeze()\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    \n",
    "    def validate_epoch_simple(model, val_loader, criterion, device):\n",
    "        \"\"\"Validation epoch for Simple Transformer\"\"\"\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_enc, x_dec, y in val_loader:\n",
    "                x_enc = x_enc.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                output = model(x_enc)\n",
    "                \n",
    "                output = output.squeeze()\n",
    "                y = y.squeeze()\n",
    "                \n",
    "                loss = criterion(output, y)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45306fe-ad32-4725-a1d8-b2b77865d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Informer-Inspired Model with ProbSparse Attention\n",
    "# Reference: Bańka & Chudziak (2025), Section 3.2.1, p. 1272-1273\n",
    "# Reference: Zhou et al. (2021), \"Informer\" original paper\n",
    "\n",
    "class ProbSparseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified ProbSparse Self-Attention mechanism.\n",
    "    \n",
    "    Reference: Bańka & Chudziak (2025), Section 3.2.1, p. 1272\n",
    "    \"ProbSparse Self-Attention Mechanism... selects a subset of queries based on \n",
    "    the Kullback-Leibler divergence (KLD)\"\n",
    "    \n",
    "    Reference: Figure 3, p. 1273 - Illustration of ProbSparse Attention mechanism\n",
    "    \n",
    "    Note: This is a simplified implementation focusing on the key concepts\n",
    "    rather than full complexity of the original Informer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1, factor=3):\n",
    "        super(ProbSparseSelfAttention, self).__init__()\n",
    "        # assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.factor = factor\n",
    "        \n",
    "        self.query_projection = nn.Linear(d_model, d_model)\n",
    "        self.key_projection = nn.Linear(d_model, d_model)\n",
    "        self.value_projection = nn.Linear(d_model, d_model)\n",
    "        self.out_projection = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = np.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries: (batch_size, seq_len, d_model)\n",
    "            keys: (batch_size, seq_len, d_model)\n",
    "            values: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = queries.size(0)\n",
    "        seq_len = queries.size(1)\n",
    "        \n",
    "        # Project and reshape for multi-head attention\n",
    "        Q = self.query_projection(queries).view(batch_size, seq_len, \n",
    "                                                 self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.key_projection(keys).view(batch_size, seq_len, \n",
    "                                           self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.value_projection(values).view(batch_size, seq_len, \n",
    "                                               self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Simplified: Use standard attention instead of full ProbSparse implementation\n",
    "        # Full implementation would select top-U queries based on sparsity measure\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if attn_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(attn_mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        output = self.out_projection(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class InformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Informer Encoder Layer with ProbSparse attention and distilling.\n",
    "    \n",
    "    Reference: Bańka & Chudziak (2025), Section 3.2.1, p. 1272-1273\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, factor=3):\n",
    "        super(InformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention = ProbSparseSelfAttention(d_model, n_heads, dropout, factor)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x, attn_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class InformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Informer-inspired model for option pricing.\n",
    "    \n",
    "    Reference: Bańka & Chudziak (2025), Section 3, p. 1270-1274\n",
    "    Full architecture description including encoder-decoder structure.\n",
    "    \n",
    "    Reference: Figure 2, p. 1272 - Complete Informer model overview\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, d_model=32, n_heads=3, \n",
    "                 n_encoder_layers=1, n_decoder_layers=2,\n",
    "                 d_ff=8, dropout=0.06, factor=3):\n",
    "        super(InformerModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input embedding - Reference: Section 3.2.1, \"Embedding Layer\", p. 1272\n",
    "        self.input_embedding = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            InformerEncoderLayer(d_model, n_heads, d_ff, dropout, factor)\n",
    "            for _ in range(n_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder setup (simplified for single-step prediction)\n",
    "        self.decoder_input = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, n_decoder_layers)\n",
    "        \n",
    "        # Output projection - Reference: Section 3.2.2, \"Decoder Output\", p. 1273\n",
    "        self.output_projection = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x_enc, x_dec):\n",
    "        \"\"\"\n",
    "        Forward pass with encoder and decoder inputs.\n",
    "        \n",
    "        Args:\n",
    "            x_enc: Encoder input (batch_size, seq_len, n_features)\n",
    "            x_dec: Decoder input (batch_size, label_len + pred_len, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            predictions: (batch_size, 1) - predicted option price\n",
    "        \"\"\"\n",
    "        # Embed encoder input\n",
    "        enc_out = self.input_embedding(x_enc)  # (batch_size, seq_len, d_model)\n",
    "        enc_out = self.pos_encoder(enc_out)\n",
    "        \n",
    "        # Pass through encoder layers (ProbSparse attention)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out)\n",
    "        \n",
    "        # Embed decoder input\n",
    "        # dec_out = self.output_projection(x_dec)  # (batch_size, label_len+pred_len, d_model)\n",
    "        dec_out = self.input_embedding(x_dec)\n",
    "        dec_out = self.pos_encoder(dec_out)\n",
    "        \n",
    "        # Pass through decoder (uses encoder output as memory)\n",
    "        dec_out = self.decoder(dec_out, enc_out)\n",
    "        \n",
    "        # Output projection - take the last timestep for prediction\n",
    "        predictions = self.output_projection(dec_out[:, -1, :])  # (batch_size, 1)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff51b7-caaa-4267-8096-3501c0c9ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Functions\n",
    "# Reference: Pimentel et al. (2025), Section 4.2.1, p. [training section]\n",
    "# Reference: Bańka & Chudziak (2025), Section 4.2, p. 1274\n",
    "\n",
    "def train_epoch_informer(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Training epoch for Informer model\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    num_batches = 0\n",
    "    update_freq = 1000\n",
    "    pbar = tqdm(train_loader, desc='Training', mininterval=10)\n",
    "        \n",
    "    # for x_enc, x_dec, y in pbar:\n",
    "    for batch_idx, (x_enc, x_dec, y) in enumerate(pbar):\n",
    "        x_enc = x_enc.to(device)\n",
    "        x_dec = x_dec.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Check for NaN in inputs\n",
    "        if torch.isnan(x_enc).any() or torch.isnan(x_dec).any() or torch.isnan(y).any():\n",
    "            print(f\"Warning: NaN detected in input at batch {batch_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Check for Inf in inputs\n",
    "        if torch.isinf(x_enc).any() or torch.isinf(x_dec).any() or torch.isinf(y).any():\n",
    "            print(f\"Warning: Inf detected in input at batch {batch_idx}\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            output = model(x_enc, x_dec)\n",
    "\n",
    "            # Check for NaN in output\n",
    "            if torch.isnan(output).any():\n",
    "                print(f\"Warning: NaN in model output at batch {batch_idx}\")\n",
    "                # Skip this batch\n",
    "                continue\n",
    "        \n",
    "            # Squeeze dimensions to match\n",
    "            output = output.squeeze()\n",
    "            y = y.squeeze()\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            # Check if loss is NaN or Inf\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"\\nWarning: Invalid loss at batch {batch_idx}: {loss.item()}\")\n",
    "                print(f\"  Output range: [{output.min().item():.6f}, {output.max().item():.6f}]\")\n",
    "                print(f\"  Target range: [{y.min().item():.6f}, {y.max().item():.6f}]\")\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "        \n",
    "            # Check for NaN in gradients before clipping\n",
    "            has_nan_grad = False\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"\\nWarning: NaN/Inf gradient in {name} at batch {batch_idx}\")\n",
    "                        has_nan_grad = True\n",
    "                        break\n",
    "            \n",
    "            if has_nan_grad:\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"\\nRuntime error at batch {batch_idx}: {str(e)}\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        # Update every update_freq batches or on last batch\n",
    "        if (batch_idx + 1) % update_freq == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            if num_batches > 0:\n",
    "                # avg_running = running_loss / min(update_freq, num_batches % update_freq or update_freq)\n",
    "                avg_total = total_loss / num_batches\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'batch': f'{batch_idx + 1}/{len(train_loader)}',\n",
    "                    'loss': f'{batch_loss:.6f}',\n",
    "                    # f'avg_{update_freq}': f'{avg_running:.6f}',\n",
    "                    'total_avg': f'{avg_total:.6f}'\n",
    "                })\n",
    "            # running_loss = 0\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        print(\"\\nWarning: No valid batches processed!\")\n",
    "        return float('nan')\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_epoch_informer(model, val_loader, criterion, device):\n",
    "    \"\"\"Validation epoch for Informer model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc='Validation', mininterval=10)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y in pbar:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            output = model(x_enc, x_dec)\n",
    "            output = output.squeeze()\n",
    "            y = y.squeeze()\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "            \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def train_epoch_simple(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Training epoch for Simple Transformer\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    num_batches = 0\n",
    "    update_freq = 1000\n",
    "    pbar = tqdm(train_loader, desc='Training', mininterval=10)\n",
    "    \n",
    "    # for x, y in pbar:\n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Check for NaN in inputs\n",
    "        if torch.isnan(x_enc).any() or torch.isnan(x_dec).any() or torch.isnan(y).any():\n",
    "            print(f\"Warning: NaN detected in input at batch {batch_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Check for Inf in inputs\n",
    "        if torch.isinf(x_enc).any() or torch.isinf(x_dec).any() or torch.isinf(y).any():\n",
    "            print(f\"Warning: Inf detected in input at batch {batch_idx}\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            output = model(x)\n",
    "        \n",
    "            # Check for NaN in output\n",
    "            if torch.isnan(output).any():\n",
    "                print(f\"Warning: NaN in model output at batch {batch_idx}\")\n",
    "                # Skip this batch\n",
    "                continue\n",
    "        \n",
    "            # Squeeze dimensions to match\n",
    "            output = output.squeeze()\n",
    "            y = y.squeeze()\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # Check for NaN in gradients before clipping\n",
    "            has_nan_grad = False\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"\\nWarning: NaN/Inf gradient in {name} at batch {batch_idx}\")\n",
    "                        has_nan_grad = True\n",
    "                        break\n",
    "            \n",
    "            if has_nan_grad:\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "                \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "            batch_loss = loss.item()\n",
    "            total_loss += batch_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"\\nRuntime error at batch {batch_idx}: {str(e)}\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        # Update every update_freq batches or on last batch\n",
    "        if (batch_idx + 1) % update_freq == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            if num_batches > 0:\n",
    "                # avg_running = running_loss / min(update_freq, num_batches % update_freq or update_freq)\n",
    "                avg_total = total_loss / num_batches\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'batch': f'{batch_idx + 1}/{len(train_loader)}',\n",
    "                    'loss': f'{batch_loss:.6f}',\n",
    "                    # f'avg_{update_freq}': f'{avg_running:.6f}',\n",
    "                    'total_avg': f'{avg_total:.6f}'\n",
    "                })\n",
    "            # running_loss = 0\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        print(\"\\nWarning: No valid batches processed!\")\n",
    "        return float('nan')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_epoch_simple(model, val_loader, criterion, device):\n",
    "    \"\"\"Validation epoch for Simple Transformer\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    num_batches = 0\n",
    "    update_freq = 1000\n",
    "    pbar = tqdm(val_loader, desc='Validation', mininterval=10)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in pbar:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            output = model(x)\n",
    "            \n",
    "            output = output.squeeze()\n",
    "            y = y.squeeze()\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.6f}'})\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config, device, model_type='informer'):\n",
    "    \"\"\"\n",
    "    Full training loop with early stopping.\n",
    "    \n",
    "    Reference: Bańka & Chudziak (2025), Section 4.2, p. 1274\n",
    "    \"Training proceeds over 300 epochs, with early stopping applied based on \n",
    "    validation loss, using a patience of 30 epochs\"\n",
    "    \n",
    "    Reference: Pimentel et al. (2025), Section 4.2.1, p. [training details]\n",
    "    \"We utilize early stopping once validation loss has not improved for 20 \n",
    "    consecutive epochs\"\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5,      # Reduce LR by half\n",
    "        patience=10,      # After 10 epochs without improvement\n",
    "        verbose=True,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # Select appropriate training functions\n",
    "    if model_type == 'informer':\n",
    "        train_fn = train_epoch_informer\n",
    "        val_fn = validate_epoch_informer\n",
    "    else:\n",
    "        train_fn = train_epoch_simple\n",
    "        val_fn = validate_epoch_simple\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"\\nTraining on {device}...\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "    print(f\"Validation batches per epoch: {len(val_loader)}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    epoch_pbar = tqdm(range(config['epochs']), desc='Overall Progress')\n",
    "    \n",
    "    # for epoch in range(config['epochs']):\n",
    "    for epoch in epoch_pbar:\n",
    "        train_loss = train_fn(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = val_fn(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            improvement = \"Y\"\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            improvement = \"\"\n",
    "\n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.6f}',\n",
    "            'val_loss': f'{val_loss:.6f}',\n",
    "            'best': f'{best_val_loss:.6f}',\n",
    "            'patience': f'{patience_counter}/{config[\"early_stopping_patience\"]}',\n",
    "            'lr': f'{optimizer.param_groups[0]['lr']:.2e}',\n",
    "            'improved': improvement\n",
    "        })\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            tqdm.write(f\"\\n{'='*70}\")\n",
    "            tqdm.write(f\"Epoch {epoch+1}/{config['epochs']} Summary:\")\n",
    "            tqdm.write(f\"  Train Loss: {train_loss:.6f}\")\n",
    "            tqdm.write(f\"  Val Loss:   {val_loss:.6f}\")\n",
    "            tqdm.write(f\"  Best Val:   {best_val_loss:.6f}\")\n",
    "            tqdm.write(f\"  Patience:   {patience_counter}/{config['early_stopping_patience']}\")\n",
    "            if improvement:\n",
    "                tqdm.write(f\"  Status:     Y Improvement!\")\n",
    "            tqdm.write(f\"{'='*70}\\n\")\n",
    "\n",
    "        if patience_counter >= config['early_stopping_patience']:\n",
    "            tqdm.write(f\"\\n{'='*70}\")\n",
    "            tqdm.write(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            tqdm.write(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "            tqdm.write(f\"{'='*70}\\n\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'epochs_trained': epoch + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f509f-fb44-470f-880b-b110864f44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Evaluation Functions\n",
    "# Reference: Pimentel et al. (2025), Table 4, p. [results tables]\n",
    "# Reference: Bańka & Chudziak (2025), Section 4.3, p. 1274\n",
    "\n",
    "def evaluate_model_informer(model, test_loader, scaler, device, feature_cols, target_col):\n",
    "    \"\"\"\n",
    "    Evaluate Informer model on test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y in test_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            x_dec = x_dec.to(device)\n",
    "            predictions = model(x_enc, x_dec)\n",
    "            \n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(y.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    predictions_scaled = np.concatenate(all_predictions, axis=0).reshape(-1, 1)\n",
    "    targets_scaled = np.concatenate(all_targets, axis=0).reshape(-1, 1)\n",
    "    \n",
    "    # Create dummy data for inverse transform (scaler expects all features)\n",
    "    n_features = len(feature_cols)\n",
    "    dummy_features = np.zeros((len(predictions_scaled), n_features))\n",
    "    target_idx = feature_cols.index(target_col) if target_col in feature_cols else -1\n",
    "    \n",
    "    # If target is in features, use proper inverse transform\n",
    "    if target_idx >= 0:\n",
    "        dummy_features[:, target_idx] = predictions_scaled.flatten()\n",
    "        predictions = scaler.inverse_transform(dummy_features)[:, target_idx].reshape(-1, 1)\n",
    "        \n",
    "        dummy_features[:, target_idx] = targets_scaled.flatten()\n",
    "        targets = scaler.inverse_transform(dummy_features)[:, target_idx].reshape(-1, 1)\n",
    "    else:\n",
    "        # Target not in feature scaling, just use scaled values\n",
    "        predictions = predictions_scaled\n",
    "        targets = targets_scaled\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    \n",
    "    theil_u1 = np.sqrt(mse) / np.sqrt(np.mean(targets**2))\n",
    "    \n",
    "    results = {\n",
    "        'predictions': predictions,\n",
    "        'targets': targets,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'theil_u1': theil_u1\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model_simple(model, test_loader, scaler, device, feature_cols, target_col):\n",
    "    \"\"\"\n",
    "    Evaluate Simple Transformer model on test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_enc, x_dec, y in test_loader:\n",
    "            x_enc = x_enc.to(device)\n",
    "            predictions = model(x_enc)\n",
    "            \n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(y.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    predictions_scaled = np.concatenate(all_predictions, axis=0).reshape(-1, 1)\n",
    "    targets_scaled = np.concatenate(all_targets, axis=0).reshape(-1, 1)\n",
    "    \n",
    "    # Same inverse transform logic as above\n",
    "    n_features = len(feature_cols)\n",
    "    dummy_features = np.zeros((len(predictions_scaled), n_features))\n",
    "    target_idx = feature_cols.index(target_col) if target_col in feature_cols else -1\n",
    "    \n",
    "    if target_idx >= 0:\n",
    "        dummy_features[:, target_idx] = predictions_scaled.flatten()\n",
    "        predictions = scaler.inverse_transform(dummy_features)[:, target_idx].reshape(-1, 1)\n",
    "        \n",
    "        dummy_features[:, target_idx] = targets_scaled.flatten()\n",
    "        targets = scaler.inverse_transform(dummy_features)[:, target_idx].reshape(-1, 1)\n",
    "    else:\n",
    "        predictions = predictions_scaled\n",
    "        targets = targets_scaled\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    theil_u1 = np.sqrt(mse) / np.sqrt(np.mean(targets**2))\n",
    "    \n",
    "    results = {\n",
    "        'predictions': predictions,\n",
    "        'targets': targets,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'theil_u1': theil_u1\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_detailed_metrics(predictions, targets):\n",
    "    \"\"\"\n",
    "    Calculate additional performance metrics.\n",
    "    \n",
    "    Reference: Pimentel et al. (2025), Table 5, p. [bias/variance table]\n",
    "    \"bias, variance and covariance proportion\"\n",
    "    \"\"\"\n",
    "    # Bias proportion\n",
    "    mean_error = np.mean(predictions - targets)\n",
    "    mse = np.mean((predictions - targets)**2)\n",
    "    bias_prop = (mean_error**2) / mse if mse > 0 else 0\n",
    "    \n",
    "    # Variance proportion\n",
    "    std_pred = np.std(predictions)\n",
    "    std_target = np.std(targets)\n",
    "    var_prop = ((std_pred - std_target)**2) / mse if mse > 0 else 0\n",
    "    \n",
    "    # Covariance proportion\n",
    "    corr = np.corrcoef(predictions.flatten(), targets.flatten())[0, 1]\n",
    "    cov_prop = 2 * (1 - corr) * std_pred * std_target / mse if mse > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'bias_proportion': bias_prop,\n",
    "        'variance_proportion': var_prop,\n",
    "        'covariance_proportion': cov_prop\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd010fee-d224-4ffc-a827-460ad61c4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualization Functions\n",
    "\n",
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss over epochs.\n",
    "    \n",
    "    Reference: Similar to Figure 5 in Pimentel et al. (2025), p. [hyperparameter search figure]\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    ax.plot(epochs, history['train_losses'], label='Training Loss', linewidth=2)\n",
    "    ax.plot(epochs, history['val_losses'], label='Validation Loss', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions_vs_actual(results, sample_size=1000, title='Predictions vs Actual'):\n",
    "    \"\"\"\n",
    "    Plot predicted vs actual option prices.\n",
    "    \n",
    "    Reference: Similar to Figures 5-7 in Bańka & Chudziak (2025), p. 1276\n",
    "    \"\"\"\n",
    "    predictions = results['predictions'].flatten()\n",
    "    targets = results['targets'].flatten()\n",
    "    \n",
    "    # Sample for visualization if dataset is large\n",
    "    if len(predictions) > sample_size:\n",
    "        indices = np.random.choice(len(predictions), sample_size, replace=False)\n",
    "        predictions_sample = predictions[indices]\n",
    "        targets_sample = targets[indices]\n",
    "    else:\n",
    "        predictions_sample = predictions\n",
    "        targets_sample = targets\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax1.scatter(targets_sample, predictions_sample, alpha=0.5, s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(targets_sample.min(), predictions_sample.min())\n",
    "    max_val = max(targets_sample.max(), predictions_sample.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax1.set_xlabel('Actual Option Price ($)', fontsize=12)\n",
    "    ax1.set_ylabel('Predicted Option Price ($)', fontsize=12)\n",
    "    ax1.set_title('Predicted vs Actual Prices', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals histogram\n",
    "    residuals = predictions - targets\n",
    "    ax2.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('Prediction Error ($)', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Distribution of Prediction Errors', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_error_by_moneyness(results, test_df, title='Error Analysis by Moneyness'):\n",
    "    \"\"\"\n",
    "    Analyze prediction errors across different moneyness levels.\n",
    "    \n",
    "    Reference: Pimentel et al. (2025), Figure 10, p. [moneyness analysis figure]\n",
    "    \"RMSE of model predictions sorted by moneyness\"\n",
    "    \"\"\"\n",
    "    predictions = results['predictions'].flatten()\n",
    "    targets = results['targets'].flatten()\n",
    "    errors = np.abs(predictions - targets)\n",
    "    \n",
    "    # Calculate moneyness\n",
    "    moneyness = test_df['UNDERLYING_LAST'].values / test_df['STRIKE'].values\n",
    "    \n",
    "    # Create moneyness bins\n",
    "    # Reference: Pimentel et al. (2025), Table 1, p. [data description]\n",
    "    # Moneyness categories: <0.97 (OTM), 0.97-1.03 (ATM), >1.03 (ITM)\n",
    "    bins = [0, 0.97, 1.03, np.inf]\n",
    "    labels = ['OTM (<0.97)', 'ATM (0.97-1.03)', 'ITM (>1.03)']\n",
    "    moneyness_categories = pd.cut(moneyness, bins=bins, labels=labels)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    error_df = pd.DataFrame({\n",
    "        'moneyness': moneyness,\n",
    "        'category': moneyness_categories,\n",
    "        'error': errors,\n",
    "        'target': targets\n",
    "    })\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Box plot by category\n",
    "    error_df.boxplot(column='error', by='category', ax=ax1)\n",
    "    ax1.set_xlabel('Moneyness Category', fontsize=12)\n",
    "    ax1.set_ylabel('Absolute Error ($)', fontsize=12)\n",
    "    ax1.set_title('Prediction Errors by Moneyness', fontsize=14, fontweight='bold')\n",
    "    plt.sca(ax1)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Average metrics by category\n",
    "    metrics_by_category = error_df.groupby('category').agg({\n",
    "        'error': ['mean', 'std', 'count']\n",
    "    }).round(2)\n",
    "    \n",
    "    categories = metrics_by_category.index\n",
    "    means = metrics_by_category['error']['mean']\n",
    "    stds = metrics_by_category['error']['std']\n",
    "    \n",
    "    ax2.bar(range(len(categories)), means, yerr=stds, capsize=5, alpha=0.7)\n",
    "    ax2.set_xticks(range(len(categories)))\n",
    "    ax2.set_xticklabels(categories, rotation=0)\n",
    "    ax2.set_xlabel('Moneyness Category', fontsize=12)\n",
    "    ax2.set_ylabel('Mean Absolute Error ($)', fontsize=12)\n",
    "    ax2.set_title('Average Error by Moneyness', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nError Statistics by Moneyness:\")\n",
    "    print(metrics_by_category)\n",
    "\n",
    "\n",
    "def plot_error_by_maturity(results, test_df, title='Error Analysis by Time to Maturity'):\n",
    "    \"\"\"\n",
    "    Analyze prediction errors across different maturity periods.\n",
    "    \n",
    "    Reference: Pimentel et al. (2025), Figure 9, p. [maturity analysis figure]\n",
    "    \"RMSE of model predictions sorted by time to maturity\"\n",
    "    \"\"\"\n",
    "    predictions = results['predictions'].flatten()\n",
    "    targets = results['targets'].flatten()\n",
    "    errors = np.abs(predictions - targets)\n",
    "    \n",
    "    # Get maturity in days\n",
    "    mtm_days = test_df['MTM'].values * 30.4375  # Convert months to days\n",
    "    \n",
    "    # Create maturity bins (in days)\n",
    "    # Reference: Pimentel et al. (2025), Tables 8-9, maturity categories\n",
    "    bins = [0, 90, 180, 270, 450, 630, 1085]\n",
    "    labels = ['1-90d', '91-180d', '181-270d', '271-450d', '451-630d', '631-1085d']\n",
    "    maturity_categories = pd.cut(mtm_days, bins=bins, labels=labels)\n",
    "    \n",
    "    error_df = pd.DataFrame({\n",
    "        'maturity_days': mtm_days,\n",
    "        'category': maturity_categories,\n",
    "        'error': errors,\n",
    "        'target': targets\n",
    "    })\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Box plot by category\n",
    "    error_df.boxplot(column='error', by='category', ax=ax1)\n",
    "    ax1.set_xlabel('Time to Maturity', fontsize=12)\n",
    "    ax1.set_ylabel('Absolute Error ($)', fontsize=12)\n",
    "    ax1.set_title('Prediction Errors by Maturity', fontsize=14, fontweight='bold')\n",
    "    plt.sca(ax1)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Average metrics by category\n",
    "    metrics_by_category = error_df.groupby('category').agg({\n",
    "        'error': ['mean', 'std', 'count']\n",
    "    }).round(2)\n",
    "    \n",
    "    categories = metrics_by_category.index\n",
    "    means = metrics_by_category['error']['mean']\n",
    "    stds = metrics_by_category['error']['std']\n",
    "    \n",
    "    x_pos = range(len(categories))\n",
    "    ax2.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(categories, rotation=45, ha='right')\n",
    "    ax2.set_xlabel('Time to Maturity', fontsize=12)\n",
    "    ax2.set_ylabel('Mean Absolute Error ($)', fontsize=12)\n",
    "    ax2.set_title('Average Error by Maturity', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nError Statistics by Maturity:\")\n",
    "    print(metrics_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e173d87-93b5-4578-9f51-9c8843859eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Hyperparameter Search Scaffolding\n",
    "# Reference: Pimentel et al. (2025), Section 4.2.3, p. [hyperparameter search]\n",
    "# Reference: Bańka & Chudziak (2025), discussion of hyperparameter tuning\n",
    "\n",
    "\n",
    "def random_search_hyperparameters(train_df, val_df, test_df, config, n_trials=10, model_type='informer'):\n",
    "    \"\"\"\n",
    "    Perform random search for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HYPERPARAMETER SEARCH: {n_trials} trials\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    hp_ranges = config['hp_search']\n",
    "    results = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Randomly sample hyperparameters\n",
    "        trial_config = config.copy()\n",
    "        trial_config['d_model'] = np.random.choice(hp_ranges['d_model'])\n",
    "        trial_config['n_heads'] = np.random.choice(hp_ranges['n_heads'])\n",
    "        trial_config['n_encoder_layers'] = np.random.choice(hp_ranges['n_encoder_layers'])\n",
    "        trial_config['n_decoder_layers'] = np.random.choice(hp_ranges['n_decoder_layers'])\n",
    "        trial_config['d_ff'] = np.random.choice(hp_ranges['d_ff'])\n",
    "        trial_config['dropout'] = np.random.choice(hp_ranges['dropout'])\n",
    "        trial_config['learning_rate'] = np.random.choice(hp_ranges['learning_rate'])\n",
    "        trial_config['batch_size'] = np.random.choice(hp_ranges['batch_size'])\n",
    "        \n",
    "        print(f\"\\nTrial {trial + 1}/{n_trials}\")\n",
    "        print(f\"  d_model={trial_config['d_model']}, n_heads={trial_config['n_heads']}, \"\n",
    "              f\"n_enc={trial_config['n_encoder_layers']}, n_dec={trial_config['n_decoder_layers']}\")\n",
    "        \n",
    "        # Create datasets for this trial\n",
    "        trial_train_dataset = OptionPricingDataset(\n",
    "            data_df=train_df,\n",
    "            target_col=trial_config['target_column'],\n",
    "            seq_len=trial_config['seq_len'],\n",
    "            label_len=trial_config['label_len'],\n",
    "            pred_len=trial_config['pred_len'],\n",
    "            feature_cols=trial_config['feature_columns']\n",
    "        )\n",
    "        \n",
    "        trial_val_dataset = OptionPricingDataset(\n",
    "            data_df=val_df,\n",
    "            target_col=trial_config['target_column'],\n",
    "            seq_len=trial_config['seq_len'],\n",
    "            label_len=trial_config['label_len'],\n",
    "            pred_len=trial_config['pred_len'],\n",
    "            feature_cols=trial_config['feature_columns']\n",
    "        )\n",
    "        \n",
    "        trial_train_loader = DataLoader(\n",
    "            trial_train_dataset, \n",
    "            batch_size=trial_config['batch_size'], \n",
    "            shuffle=True\n",
    "        )\n",
    "        trial_val_loader = DataLoader(\n",
    "            trial_val_dataset, \n",
    "            batch_size=trial_config['batch_size'], \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        n_features = len(trial_config['feature_columns'])\n",
    "        \n",
    "        if model_type == 'simple':\n",
    "            model = SimpleTransformerModel(\n",
    "                n_features=n_features,\n",
    "                d_model=trial_config['d_model'],\n",
    "                n_heads=trial_config['n_heads'],\n",
    "                n_layers=trial_config['n_encoder_layers'],\n",
    "                d_ff=trial_config['d_ff'],\n",
    "                dropout=trial_config['dropout']\n",
    "            ).to(device)\n",
    "        else:  # informer\n",
    "            model = InformerModel(\n",
    "                n_features=n_features,\n",
    "                d_model=trial_config['d_model'],\n",
    "                n_heads=trial_config['n_heads'],\n",
    "                n_encoder_layers=trial_config['n_encoder_layers'],\n",
    "                n_decoder_layers=trial_config['n_decoder_layers'],\n",
    "                d_ff=trial_config['d_ff'],\n",
    "                dropout=trial_config['dropout']\n",
    "            ).to(device)\n",
    "        \n",
    "        # Train with reduced epochs for search\n",
    "        search_config = trial_config.copy()\n",
    "        search_config['epochs'] = 30\n",
    "        search_config['early_stopping_patience'] = 5\n",
    "        \n",
    "        try:\n",
    "            history = train_model(model, trial_train_loader, trial_val_loader, \n",
    "                                search_config, device, model_type=model_type)\n",
    "            \n",
    "            trial_result = {\n",
    "                'trial': trial + 1,\n",
    "                'config': trial_config,\n",
    "                'val_loss': history['best_val_loss'],\n",
    "                'epochs_trained': history['epochs_trained']\n",
    "            }\n",
    "            \n",
    "            results.append(trial_result)\n",
    "            \n",
    "            print(f\"  Result: Val Loss = {history['best_val_loss']:.6f}\")\n",
    "            \n",
    "            if history['best_val_loss'] < best_val_loss:\n",
    "                best_val_loss = history['best_val_loss']\n",
    "                best_config = trial_config.copy()\n",
    "                print(f\"  *** New best configuration! ***\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Trial failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEARCH COMPLETE\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "\n",
    "def plot_hyperparameter_search_results(results):\n",
    "    \"\"\"\n",
    "    Visualize hyperparameter search results.\n",
    "    \n",
    "    Reference: Similar to Pimentel et al. (2025), Figures 5-6, p. [HP search figures]\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Extract data\n",
    "    val_losses = [r['val_loss'] for r in results]\n",
    "    d_models = [r['config']['d_model'] for r in results]\n",
    "    n_heads = [r['config']['n_heads'] for r in results]\n",
    "    learning_rates = [r['config']['learning_rate'] for r in results]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Validation losses over trials\n",
    "    axes[0, 0].plot(range(1, len(val_losses) + 1), val_losses, marker='o')\n",
    "    axes[0, 0].axhline(y=min(val_losses), color='r', linestyle='--', \n",
    "                       label=f'Best: {min(val_losses):.6f}')\n",
    "    axes[0, 0].set_xlabel('Trial Number', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Validation Loss', fontsize=12)\n",
    "    axes[0, 0].set_title('Validation Loss Across Trials', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # d_model vs validation loss\n",
    "    axes[0, 1].scatter(d_models, val_losses, alpha=0.6, s=100)\n",
    "    axes[0, 1].set_xlabel('d_model', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Validation Loss', fontsize=12)\n",
    "    axes[0, 1].set_title('Model Dimension vs Performance', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # n_heads vs validation loss\n",
    "    axes[1, 0].scatter(n_heads, val_losses, alpha=0.6, s=100)\n",
    "    axes[1, 0].set_xlabel('Number of Attention Heads', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Validation Loss', fontsize=12)\n",
    "    axes[1, 0].set_title('Attention Heads vs Performance', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate vs validation loss\n",
    "    axes[1, 1].scatter(learning_rates, val_losses, alpha=0.6, s=100)\n",
    "    axes[1, 1].set_xlabel('Learning Rate', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Validation Loss', fontsize=12)\n",
    "    axes[1, 1].set_title('Learning Rate vs Performance', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(MODEL_TYPE, model, test_loader, feature_scaler, device, CONFIG):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING {MODEL_TYPE.upper()} MODEL ON TEST SET\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    if MODEL_TYPE == 'informer':\n",
    "        results = evaluate_model_informer(\n",
    "            model, \n",
    "            test_loader, \n",
    "            feature_scaler,\n",
    "            device,\n",
    "            CONFIG['feature_columns'],\n",
    "            CONFIG['target_column']\n",
    "        )\n",
    "    else:\n",
    "        results = evaluate_model_simple(\n",
    "            model, \n",
    "            test_loader, \n",
    "            feature_scaler,\n",
    "            device,\n",
    "            CONFIG['feature_columns'],\n",
    "            CONFIG['target_column']\n",
    "        )\n",
    "    \n",
    "    # # Evaluate on test set\n",
    "    # results = evaluate_model(\n",
    "    #     model, \n",
    "    #     test_loader, \n",
    "    #     data_splits['scaler_target'], \n",
    "    #     device\n",
    "    # )\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    detailed_metrics = calculate_detailed_metrics(\n",
    "        results['predictions'], \n",
    "        results['targets']\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(f\"  RMSE: ${results['rmse']:.2f}\")\n",
    "    print(f\"  MAE: ${results['mae']:.2f}\")\n",
    "    print(f\"  MSE: {results['mse']:.2f}\")\n",
    "    print(f\"  Theil U1: {results['theil_u1']:.6f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Metrics:\")\n",
    "    print(f\"  Bias Proportion: {detailed_metrics['bias_proportion']:.4f}\")\n",
    "    print(f\"  Variance Proportion: {detailed_metrics['variance_proportion']:.4f}\")\n",
    "    print(f\"  Covariance Proportion: {detailed_metrics['covariance_proportion']:.4f}\")\n",
    "    print(f\"  Sum (should be ≈1.0): {sum(detailed_metrics.values()):.4f}\")\n",
    "    \n",
    "    # Reference: Pimentel et al. (2025), Table 4, p. [results table]\n",
    "    # Compare with benchmarks mentioned in paper\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Reference Benchmarks from Literature:\")\n",
    "    print(\"  Black-Scholes RMSE: ~$37.82 (Pimentel et al., 2025, Table 4)\")\n",
    "    print(\"  Heston RMSE: ~$35.90 (Pimentel et al., 2025, Table 4)\")\n",
    "    print(\"  LSTM RMSE: ~$27.94 (Pimentel et al., 2025, Table 4)\")\n",
    "    print(\"  Informer RMSE: ~$2.71 (Bańka & Chudziak, 2025, Table 1)\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075ed49-319c-4d77-a2ea-3a70327ca1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Main Execution - Data Preparation\n",
    "# This cell prepares data and can be run independently\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSFORMER-BASED OPTION PRICING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create data splits\n",
    "train_df, val_df, test_df, feature_scaler = create_rolling_window_split(\n",
    "    df,\n",
    "    test_month=CONFIG['test_month'],\n",
    "    test_year=CONFIG['test_year'],\n",
    "    feature_columns=CONFIG['feature_columns'],\n",
    "    target_column=CONFIG['target_column'],\n",
    "    train_months=CONFIG['train_months'],\n",
    "    val_months=CONFIG['val_months']\n",
    ")\n",
    "\n",
    "# Create datasets with actual data\n",
    "train_dataset = OptionPricingDataset(\n",
    "    data_df=train_df,\n",
    "    target_col=CONFIG['target_column'],\n",
    "    seq_len=CONFIG['seq_len'],\n",
    "    label_len=CONFIG['label_len'],\n",
    "    pred_len=CONFIG['pred_len'],\n",
    "    feature_cols=CONFIG['feature_columns']\n",
    ")\n",
    "\n",
    "val_dataset = OptionPricingDataset(\n",
    "    data_df=val_df,\n",
    "    target_col=CONFIG['target_column'],\n",
    "    seq_len=CONFIG['seq_len'],\n",
    "    label_len=CONFIG['label_len'],\n",
    "    pred_len=CONFIG['pred_len'],\n",
    "    feature_cols=CONFIG['feature_columns']\n",
    ")\n",
    "\n",
    "test_dataset = OptionPricingDataset(\n",
    "    data_df=test_df,\n",
    "    target_col=CONFIG['target_column'],\n",
    "    seq_len=CONFIG['seq_len'],\n",
    "    label_len=CONFIG['label_len'],\n",
    "    pred_len=CONFIG['pred_len'],\n",
    "    feature_cols=CONFIG['feature_columns']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Check one batch\n",
    "x_enc, x_dec, y = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  x_enc (encoder input): {x_enc.shape}\")  # (batch_size, seq_len, 5)\n",
    "print(f\"  x_dec (decoder input): {x_dec.shape}\")  # (batch_size, label_len+pred_len, 5)\n",
    "print(f\"  y (target): {y.shape}\")                 # (batch_size, pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7a7a3-6da4-4e1b-92d2-b578fada47ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 13a: Informer Model Selection and Training\n",
    "# Choose which model to train: 'simple' or 'informer'\n",
    "MODEL_TYPE = 'informer'\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TRAINING {MODEL_TYPE.upper()} MODEL\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Initialize model\n",
    "n_features = len(CONFIG['feature_columns'])\n",
    "\n",
    "if MODEL_TYPE == 'simple':\n",
    "    # Reference: Ruiru et al. (2024), Section 3.2, p. 545\n",
    "    model = SimpleTransformerModel(\n",
    "        n_features=n_features,\n",
    "        d_model=CONFIG['d_model'],\n",
    "        n_heads=CONFIG['n_heads'],\n",
    "        n_layers=CONFIG['n_encoder_layers'],\n",
    "        d_ff=CONFIG['d_ff'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "    print(\"Initialized Simple Transformer (Baseline)\")\n",
    "    \n",
    "else:  # informer\n",
    "    # Reference: Bańka & Chudziak (2025), Section 3, p. 1270-1274\n",
    "    model = InformerModel(\n",
    "        n_features=n_features,\n",
    "        d_model=CONFIG['d_model'],\n",
    "        n_heads=CONFIG['n_heads'],\n",
    "        n_encoder_layers=CONFIG['n_encoder_layers'],\n",
    "        n_decoder_layers=CONFIG['n_decoder_layers'],\n",
    "        d_ff=CONFIG['d_ff'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "    print(\"Initialized Informer Model\")\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Features: {n_features}\")\n",
    "print(f\"  d_model: {CONFIG['d_model']}\")\n",
    "print(f\"  Attention heads: {CONFIG['n_heads']}\")\n",
    "print(f\"  Encoder layers: {CONFIG['n_encoder_layers']}\")\n",
    "if MODEL_TYPE == 'informer':\n",
    "    print(f\"  Decoder layers: {CONFIG['n_decoder_layers']}\")\n",
    "print(f\"  Feed-forward dim: {CONFIG['d_ff']}\")\n",
    "print(f\"  Dropout: {CONFIG['dropout']}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train model\n",
    "history = train_model(model, train_loader, val_loader, CONFIG, device, model_type=MODEL_TYPE)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history, title=f'{MODEL_TYPE.upper()} Training History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fef5d-4a94-4098-a2e6-7d0302b033de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 13b: Simple Model Selection and Training\n",
    "# # Choose which model to train: 'simple' or 'informer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9660c-964c-4ffe-8904-de4e2f81c03c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 14: Model Evaluation\n",
    "# Reference: Bańka & Chudziak (2025), Section 4.4, p. 1275-1277\n",
    "# Reference: Pimentel et al. (2025), Section 5, Results section\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EVALUATING {MODEL_TYPE.upper()} MODEL ON TEST SET\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "if MODEL_TYPE == 'informer':\n",
    "    results = evaluate_model_informer(\n",
    "        model, \n",
    "        test_loader, \n",
    "        feature_scaler,\n",
    "        device,\n",
    "        CONFIG['feature_columns'],\n",
    "        CONFIG['target_column']\n",
    "    )\n",
    "else:\n",
    "    results = evaluate_model_simple(\n",
    "        model, \n",
    "        test_loader, \n",
    "        feature_scaler,\n",
    "        device,\n",
    "        CONFIG['feature_columns'],\n",
    "        CONFIG['target_column']\n",
    "    )\n",
    "\n",
    "# # Evaluate on test set\n",
    "# results = evaluate_model(\n",
    "#     model, \n",
    "#     test_loader, \n",
    "#     data_splits['scaler_target'], \n",
    "#     device\n",
    "# )\n",
    "\n",
    "# Calculate additional metrics\n",
    "detailed_metrics = calculate_detailed_metrics(\n",
    "    results['predictions'], \n",
    "    results['targets']\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Performance Metrics:\")\n",
    "print(f\"  RMSE: ${results['rmse']:.2f}\")\n",
    "print(f\"  MAE: ${results['mae']:.2f}\")\n",
    "print(f\"  MSE: {results['mse']:.2f}\")\n",
    "print(f\"  Theil U1: {results['theil_u1']:.6f}\")\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"  Bias Proportion: {detailed_metrics['bias_proportion']:.4f}\")\n",
    "print(f\"  Variance Proportion: {detailed_metrics['variance_proportion']:.4f}\")\n",
    "print(f\"  Covariance Proportion: {detailed_metrics['covariance_proportion']:.4f}\")\n",
    "print(f\"  Sum (should be ≈1.0): {sum(detailed_metrics.values()):.4f}\")\n",
    "\n",
    "# Reference: Pimentel et al. (2025), Table 4, p. [results table]\n",
    "# Compare with benchmarks mentioned in paper\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Reference Benchmarks from Literature:\")\n",
    "print(\"  Black-Scholes RMSE: ~$37.82 (Pimentel et al., 2025, Table 4)\")\n",
    "print(\"  Heston RMSE: ~$35.90 (Pimentel et al., 2025, Table 4)\")\n",
    "print(\"  LSTM RMSE: ~$27.94 (Pimentel et al., 2025, Table 4)\")\n",
    "print(\"  Informer RMSE: ~$2.71 (Bańka & Chudziak, 2025, Table 1)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d43c3-09cd-4c67-8112-ec32d3fa0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 15: Visualize Results\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"GENERATING VISUALIZATIONS\")\n",
    "# print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# # Plot predictions vs actual\n",
    "# plot_predictions_vs_actual(\n",
    "#     results, \n",
    "#     sample_size=2000,\n",
    "#     title=f'{MODEL_TYPE.upper()} Model: Predictions vs Actual'\n",
    "# )\n",
    "\n",
    "# # Analyze by moneyness\n",
    "# # Reference: Pimentel et al. (2025), Figure 10, p. [moneyness figure]\n",
    "# plot_error_by_moneyness(\n",
    "#     results, \n",
    "#     data_splits['test'][2],\n",
    "#     title=f'{MODEL_TYPE.upper()} Error Analysis by Moneyness'\n",
    "# )\n",
    "\n",
    "# # Analyze by maturity\n",
    "# # Reference: Pimentel et al. (2025), Figure 9, p. [maturity figure]\n",
    "# plot_error_by_maturity(\n",
    "#     results, \n",
    "#     data_splits['test'][2],\n",
    "#     title=f'{MODEL_TYPE.upper()} Error Analysis by Time to Maturity'\n",
    "# )\n",
    "\n",
    "\n",
    "# Cell 15: Visualize Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plot_predictions_vs_actual(\n",
    "    results, \n",
    "    sample_size=2000,\n",
    "    title=f'{MODEL_TYPE.upper()} Model: Predictions vs Actual'\n",
    ")\n",
    "\n",
    "# Analyze by moneyness\n",
    "plot_error_by_moneyness(\n",
    "    results, \n",
    "    test_df,  # ✅ Use test_df from Cell 12\n",
    "    title=f'{MODEL_TYPE.upper()} Error Analysis by Moneyness'\n",
    ")\n",
    "\n",
    "# Analyze by maturity\n",
    "plot_error_by_maturity(\n",
    "    results, \n",
    "    test_df,  # ✅ Use test_df from Cell 12\n",
    "    title=f'{MODEL_TYPE.upper()} Error Analysis by Time to Maturity'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defeac31-f2ac-4beb-8d51-ec0f9beda3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Optional - Hyperparameter Search\n",
    "# Set RUN_HP_SEARCH to True to perform hyperparameter optimization\n",
    "# Warning: This can take significant time depending on n_trials\n",
    "\n",
    "RUN_HP_SEARCH = False  # Set to True to run hyperparameter search\n",
    "N_TRIALS = 10  # Number of random configurations to try\n",
    "\n",
    "if RUN_HP_SEARCH:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING HYPERPARAMETER SEARCH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    best_config, search_results = random_search_hyperparameters(\n",
    "        data_splits,\n",
    "        CONFIG,\n",
    "        n_trials=N_TRIALS,\n",
    "        model_type=MODEL_TYPE\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBest Configuration Found:\")\n",
    "    for key, value in best_config.items():\n",
    "        if key not in ['hp_search', 'feature_columns']:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Visualize search results\n",
    "    plot_hyperparameter_search_results(search_results)\n",
    "    \n",
    "    # Save best config\n",
    "    print(\"\\nTo use best configuration, update CONFIG with these values\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nHyperparameter search skipped (RUN_HP_SEARCH = False)\")\n",
    "    print(\"To run hyperparameter search, set RUN_HP_SEARCH = True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac3aa2-44cd-4062-976a-d7d9ce1b3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Model Comparison Table\n",
    "# Reference: Pimentel et al. (2025), Tables 4-5, p. [results tables]\n",
    "# Reference: Bańka & Chudziak (2025), Tables 1-3, p. 1275\n",
    "\n",
    "def create_comparison_table(model_results_dict):\n",
    "    \"\"\"\n",
    "    Create comparison table for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        model_results_dict: Dictionary with model names as keys and results as values\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in model_results_dict.items():\n",
    "        detailed = calculate_detailed_metrics(\n",
    "            results['predictions'],\n",
    "            results['targets']\n",
    "        )\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'RMSE': results['rmse'],\n",
    "            'MAE': results['mae'],\n",
    "            'Theil U1': results['theil_u1'],\n",
    "            'Bias %': detailed['bias_proportion'] * 100,\n",
    "            'Variance %': detailed['variance_proportion'] * 100,\n",
    "            'Covariance %': detailed['covariance_proportion'] * 100\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "# Store current model results\n",
    "model_results = {\n",
    "    MODEL_TYPE.upper(): results\n",
    "}\n",
    "\n",
    "# Create and display comparison table\n",
    "comparison_table = create_comparison_table(model_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_table.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Note: To compare multiple models, train each model and add results to model_results dictionary\n",
    "print(\"\\nNote: Train multiple models and add to model_results dictionary for full comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba46a9f-5599-446f-907f-2c7fb0187de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Save Model and Results\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "SAVE_RESULTS = True  # Set to False to skip saving\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f'./results/{MODEL_TYPE}_model_{timestamp}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = f'./results/{MODEL_TYPE}_config_{timestamp}.pkl'\n",
    "    with open(config_path, 'wb') as f:\n",
    "        pickle.dump(CONFIG, f)\n",
    "    print(f\"Configuration saved to: {config_path}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = f'./results/{MODEL_TYPE}_results_{timestamp}.pkl'\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'results': results,\n",
    "            'history': history,\n",
    "            'comparison_table': comparison_table\n",
    "        }, f)\n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    \n",
    "    print(f\"\\nAll outputs saved with timestamp: {timestamp}\")\n",
    "else:\n",
    "    print(\"Saving skipped (SAVE_RESULTS = False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90715307-019d-4d9c-a67b-817ce1ad095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Summary and Key Findings\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_TYPE.upper()}\")\n",
    "print(f\"Test Period: {CONFIG['test_year']}-{CONFIG['test_month']:02d}\")\n",
    "print(f\"Training Samples: {len(train_dataset):,}\")\n",
    "print(f\"Test Samples: {len(test_dataset):,}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Reference: Bańka & Chudziak (2025), Section 5, p. 1276\n",
    "print(f\"\\n1. Prediction Accuracy:\")\n",
    "print(f\"   - RMSE: \\${results['rmse']:.2f}\")\n",
    "print(f\"   - MAE: \\${results['mae']:.2f}\")\n",
    "print(f\"   - Performance relative to Black-Scholes baseline\")\n",
    "\n",
    "# Reference: Pimentel et al. (2025), Section 5.2, p. [moneyness analysis]\n",
    "print(f\"\\n2. Performance by Moneyness:\")\n",
    "print(f\"   - Model shows varying accuracy across OTM, ATM, and ITM options\")\n",
    "print(f\"   - See moneyness analysis plots above\")\n",
    "\n",
    "# Reference: Pimentel et al. (2025), Section 5.2, p. [maturity analysis]\n",
    "print(f\"\\n3. Performance by Time to Maturity:\")\n",
    "print(f\"   - Model performance changes with option maturity\")\n",
    "print(f\"   - See maturity analysis plots above\")\n",
    "\n",
    "# Reference: Bańka & Chudziak (2025), Section 5, p. 1276\n",
    "print(f\"\\n4. Model Architecture:\")\n",
    "print(f\"   - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   - Epochs trained: {history['epochs_trained']}\")\n",
    "print(f\"   - Best validation loss: {history['best_val_loss']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REFERENCES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. Bańka, F., & Chudziak, J. A. (2025). Applying Informer for Option Pricing: \n",
    "   A Transformer-Based Approach. ICAART 2025, pp. 1270-1277.\n",
    "\n",
    "2. Pimentel, R., et al. (2025). Option pricing with deep learning: \n",
    "   a long short-term memory approach. Decisions in Economics and Finance.\n",
    "\n",
    "3. Ruiru, D. K., Jouandeau, N., & Odhiambo, D. (2024). LSTM versus Transformers: \n",
    "   A Practical Comparison of Deep Learning Models for Trading Financial Instruments.\n",
    "   IJCCI 2024, pp. 543-549.\n",
    "\n",
    "4. Zhou, H., et al. (2021). Informer: Beyond Efficient Transformer for Long \n",
    "   Sequence Time-Series Forecasting. AAAI 2021.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1f33b-ac8a-400d-b37f-7382fa3ae31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
